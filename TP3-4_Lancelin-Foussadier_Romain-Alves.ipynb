{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP3-4 IASD Lancelin Foussadier et Romain Alves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from random import randint\n",
    "mlp_clf_AND = MLPClassifier(hidden_layer_sizes=(),\n",
    "                        max_iter = 300,activation = 'identity',\n",
    "                        solver = 'lbfgs')\n",
    "X = [ [0., 0.], [0., 1.], [1., 0.], [1., 1.]] # Entrées\n",
    "y = [0, 0, 0, 1] # Résultats attendus\n",
    "mlp_clf_AND.fit(X,y)\n",
    "mlp_clf_AND.predict(X)\n",
    "X_test = []\n",
    "for i in range(60):\n",
    "    x = (randint(0,1),randint(0,1))\n",
    "    X_test.append(x)\n",
    "mlp_clf_AND.predict(X_test)\n",
    "mlp_clf_OR = MLPClassifier(hidden_layer_sizes=(),\n",
    "                        max_iter = 300,activation = 'identity',\n",
    "                        solver = 'lbfgs')\n",
    "y_OR = [0,1,1,1]\n",
    "mlp_clf_OR.fit(X,y_OR)\n",
    "mlp_clf_OR.predict(X_test)\n",
    "\n",
    "mlp_clf_XOR_0 = MLPClassifier(hidden_layer_sizes=(),\n",
    "                        max_iter = 300,activation = 'identity',\n",
    "                        solver = 'lbfgs')\n",
    "y_XOR = [0,1,1,0]\n",
    "mlp_clf_XOR_0.fit(X,y_XOR)\n",
    "mlp_clf_XOR_0.predict(X_test)\n",
    "\n",
    "#On observe que le Classifieur a eu une prédiction \"aléatoire\" tout a été prédis à 0. \n",
    "#Cela peut être due au fait que les jeux de tests n'ont pas assez entrainés le modèle\n",
    "mlp_clf_XOR_1 = MLPClassifier(hidden_layer_sizes=(4,2),\n",
    "                        max_iter = 300,activation = 'identity',\n",
    "                        solver = 'lbfgs')\n",
    "\n",
    "\n",
    "### On modifie ici le pour tester AND, OR et XOR\n",
    "mlp_clf_XOR_1.fit(X,y_XOR)\n",
    "y_test = mlp_clf_XOR_1.predict(X_test)\n",
    "\n",
    "x_model =[]\n",
    "for i in X_test:\n",
    "    if (i[0] == 1) and (i[1] == 1):\n",
    "        x_model.append(0)\n",
    "    elif  i[0] == 0 and i[1] == 0:\n",
    "        x_model.append(0)\n",
    "    else :\n",
    "        x_model.append(1)\n"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:27:25.430327548Z",
     "start_time": "2023-10-16T15:27:25.267782992Z"
    }
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(x_model,y_test))\n",
    "print(x_model)\n",
    "print(y_test)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:27:25.487405549Z",
     "start_time": "2023-10-16T15:27:25.300817876Z"
    }
   },
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]\n",
      "[0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1\n",
      " 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On remarque que le calssifier prédis la bonne valeure entre 50 et 80% du temps pour chaque opération logique."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mlp_clf_XOR_2 = MLPClassifier(hidden_layer_sizes=(4,2),activation = 'tanh',\n",
    "                        solver = 'lbfgs')\n",
    "mlp_clf_XOR_2.fit(X,y_XOR)\n",
    "y_test = mlp_clf_XOR_2.predict(X_test)\n",
    "accuracy_score(x_model,y_test)\n",
    "#Les résultats sont bons dans 100% des cas même après plusieurs apprentissages"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:27:25.487855975Z",
     "start_time": "2023-10-16T15:27:25.342301063Z"
    }
   },
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sans neurones les résultats sont très mauvais, ils sont aléatoires entre 80% et 20%. Avec 4 neurones par couche et deux couches les resultats s'améliorent peu. Et avec tanh les résultats sont très bons. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_digits \n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_digits()\n",
    "X = dataset.data # I n p u t s\n",
    "y = dataset.target # A s s o c i a t e d o u t p u t s\n",
    "train_X , test_X , train_y , test_y = train_test_split (X, y, test_size =0.1)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100),max_iter = 1000, activation = 'identity', solver = 'lbfgs')\n",
    "mlp.fit(train_X,train_y)\n",
    "y_model = mlp.predict(test_X)\n",
    "accuracy_score(y_model,test_y)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:27:25.602308389Z",
     "start_time": "2023-10-16T15:27:25.342972026Z"
    }
   },
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9611111111111111"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Après plusieurs tests, on peut estimer que les meilleurs paramètres sont 'identity', 'lbfgs' et 100 couches de neurones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "[1, 100] loss: 2.289447727203369\n",
      "[1, 200] loss: 2.247607374191284\n",
      "[1, 300] loss: 2.0605725729465485\n",
      "[1, 400] loss: 1.1821289372444153\n",
      "[1, 500] loss: 0.5977187219262123\n",
      "[1, 600] loss: 0.4634158892929554\n",
      "[1, 700] loss: 0.39198525846004484\n",
      "[1, 800] loss: 0.39240347757935523\n",
      "[1, 900] loss: 0.36521195456385613\n",
      "[2, 100] loss: 0.31423467613756656\n",
      "[2, 200] loss: 0.2762450975179672\n",
      "[2, 300] loss: 0.2678286175429821\n",
      "[2, 400] loss: 0.26454481422901155\n",
      "[2, 500] loss: 0.2355415190011263\n",
      "[2, 600] loss: 0.2312073064595461\n",
      "[2, 700] loss: 0.22192187286913395\n",
      "[2, 800] loss: 0.19909939561039208\n",
      "[2, 900] loss: 0.1872195026278496\n",
      "Accuracy of the network on the 10000 test images: 94 %\n",
      "Accuracy for class: 0     is 98.1 %\n",
      "Accuracy for class: 1     is 97.5 %\n",
      "Accuracy for class: 2     is 85.8 %\n",
      "Accuracy for class: 3     is 90.6 %\n",
      "Accuracy for class: 4     is 95.5 %\n",
      "Accuracy for class: 5     is 97.1 %\n",
      "Accuracy for class: 6     is 94.8 %\n",
      "Accuracy for class: 7     is 97.4 %\n",
      "Accuracy for class: 8     is 95.3 %\n",
      "Accuracy for class: 9     is 90.1 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1309,), (0.3084,)) # Moyenne et ecart-type calculées auparavant\n",
    "])\n",
    "trainset = datasets.MNIST(root='./data', train=True,download=True, transform=transform) # On crée un dataset\n",
    "trainloader = DataLoader(trainset, batch_size=64,shuffle=True) # On crée un dataloader\n",
    "testset = datasets.MNIST(root='./data', train=False,download=True, transform=transform) # On crée un dataset\n",
    "testloader = DataLoader(testset, batch_size=64,shuffle=False) # On crée un dataloader\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader): # On itère sur les batchs\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad() # On met les gradients à 0\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs) # On calcule les sorties du réseau\n",
    "        loss = criterion(outputs, labels) # On calcule la loss\n",
    "        loss.backward() # On calcule les gradients\n",
    "        optimizer.step() # On met à jour les poids\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99: # On affiche la loss tous les 100 batchs\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 0.3084 + 0.1309     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calcule de la précision (accuracy) sur le jeu de test\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "# Accuracy par classe\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:27:41.060161734Z",
     "start_time": "2023-10-16T15:27:25.598337906Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On remarque que la précision est entre 95 et 98%  ce qui est très bon comparé au classifier qui lui oscillait entre 85 et 96%.\n",
    "Nous allons maintenant tester de faire varié les paramètres du réseau de neurones pour voir si on peut améliorer la précision comme on l'avait fait pour le classifier.\n",
    "En commenceant par la taille des noyaux de convolution. Que l'on va faire varier à 3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "[1, 100] loss: 2.296995027065277\n",
      "[1, 200] loss: 2.26899742603302\n",
      "[1, 300] loss: 2.19838663816452\n",
      "[1, 400] loss: 1.8708905899524688\n",
      "[1, 500] loss: 1.0042804646492005\n",
      "[1, 600] loss: 0.6021834731101989\n",
      "[1, 700] loss: 0.4854024292528629\n",
      "[1, 800] loss: 0.41560176491737366\n",
      "[1, 900] loss: 0.363036797195673\n",
      "[2, 100] loss: 0.30029556527733803\n",
      "[2, 200] loss: 0.2763645215332508\n",
      "[2, 300] loss: 0.2653337824344635\n",
      "[2, 400] loss: 0.23153631981462242\n",
      "[2, 500] loss: 0.2309646026790142\n",
      "[2, 600] loss: 0.20986890681087972\n",
      "[2, 700] loss: 0.20417704679071902\n",
      "[2, 800] loss: 0.21192378524690866\n",
      "[2, 900] loss: 0.16827741261571647\n",
      "Accuracy of the network on the 10000 test images: 94 %\n",
      "Accuracy for class: 0     is 99.0 %\n",
      "Accuracy for class: 1     is 95.9 %\n",
      "Accuracy for class: 2     is 93.2 %\n",
      "Accuracy for class: 3     is 96.3 %\n",
      "Accuracy for class: 4     is 89.6 %\n",
      "Accuracy for class: 5     is 95.9 %\n",
      "Accuracy for class: 6     is 93.8 %\n",
      "Accuracy for class: 7     is 85.2 %\n",
      "Accuracy for class: 8     is 95.1 %\n",
      "Accuracy for class: 9     is 96.2 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net2 = Net()\n",
    "print(net2)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1309,), (0.3084,)) # Moyenne et ecart-type calculées auparavant\n",
    "])\n",
    "trainset = datasets.MNIST(root='./data', train=True,download=True, transform=transform) # On crée un dataset\n",
    "trainloader = DataLoader(trainset, batch_size=64,shuffle=True) # On crée un dataloader\n",
    "testset = datasets.MNIST(root='./data', train=False,download=True, transform=transform) # On crée un dataset\n",
    "testloader = DataLoader(testset, batch_size=64,shuffle=False) # On crée un dataloader\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader): # On itère sur les batchs\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad() # On met les gradients à 0\n",
    "        # forward + backward + optimize\n",
    "        outputs = net2(inputs) # On calcule les sorties du réseau\n",
    "        loss = criterion(outputs, labels) # On calcule la loss\n",
    "        loss.backward() # On calcule les gradients\n",
    "        optimizer.step() # On met à jour les poids\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99: # On affiche la loss tous les 100 batchs\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 0.3084 + 0.1309     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calcule de la précision (accuracy) sur le jeu de test\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net2(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "# Accuracy par classe\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net2(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:28:06.511046498Z",
     "start_time": "2023-10-16T15:27:41.065238591Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On voit qu'avec des noyaux de taille moins grand le réseau de neurones est moins performant.\n",
    "On va maintenant essayer de faire varier la taille des features maps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 25, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(25, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "[1, 100] loss: 2.294354889392853\n",
      "[1, 200] loss: 2.2426527094841004\n",
      "[1, 300] loss: 1.9542124843597413\n",
      "[1, 400] loss: 0.9202047330141068\n",
      "[1, 500] loss: 0.5156365664303303\n",
      "[1, 600] loss: 0.4043690373003483\n",
      "[1, 700] loss: 0.35698885187506674\n",
      "[1, 800] loss: 0.2991925658285618\n",
      "[1, 900] loss: 0.2642312593013048\n",
      "[2, 100] loss: 0.23278339177370072\n",
      "[2, 200] loss: 0.23320513352751732\n",
      "[2, 300] loss: 0.20033549554646016\n",
      "[2, 400] loss: 0.19469706673175097\n",
      "[2, 500] loss: 0.1694080750271678\n",
      "[2, 600] loss: 0.16031047824770212\n",
      "[2, 700] loss: 0.1692545835673809\n",
      "[2, 800] loss: 0.1524422680027783\n",
      "[2, 900] loss: 0.1538460937514901\n",
      "Accuracy of the network on the 10000 test images: 96 %\n",
      "Accuracy for class: 0     is 97.4 %\n",
      "Accuracy for class: 1     is 98.8 %\n",
      "Accuracy for class: 2     is 95.6 %\n",
      "Accuracy for class: 3     is 96.9 %\n",
      "Accuracy for class: 4     is 95.9 %\n",
      "Accuracy for class: 5     is 95.1 %\n",
      "Accuracy for class: 6     is 97.9 %\n",
      "Accuracy for class: 7     is 92.0 %\n",
      "Accuracy for class: 8     is 97.6 %\n",
      "Accuracy for class: 9     is 94.6 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 25, 5)\n",
    "        self.conv2 = nn.Conv2d(25, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net3 = Net()\n",
    "print(net3)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1309,), (0.3084,)) # Moyenne et ecart-type calculées auparavant\n",
    "])\n",
    "trainset = datasets.MNIST(root='./data', train=True,download=True, transform=transform) # On crée un dataset\n",
    "trainloader = DataLoader(trainset, batch_size=64,shuffle=True) # On crée un dataloader\n",
    "testset = datasets.MNIST(root='./data', train=False,download=True, transform=transform) # On crée un dataset\n",
    "testloader = DataLoader(testset, batch_size=64,shuffle=False) # On crée un dataloader\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net3.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader): # On itère sur les batchs\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad() # On met les gradients à 0\n",
    "        # forward + backward + optimize\n",
    "        outputs = net3(inputs) # On calcule les sorties du réseau\n",
    "        loss = criterion(outputs, labels) # On calcule la loss\n",
    "        loss.backward() # On calcule les gradients\n",
    "        optimizer.step() # On met à jour les poids\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99: # On affiche la loss tous les 100 batchs\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 0.3084 + 0.1309     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calcule de la précision (accuracy) sur le jeu de test\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net3(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "# Accuracy par classe\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net3(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:28:26.977525042Z",
     "start_time": "2023-10-16T15:28:06.518464859Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On voit qu'avec des features maps de taille plus petite le réseau de neurones est moins performant. Néanmoins si l'on augmente on a peu de variation de la précision.\n",
    "On va maintenant essayer de faire varier le nombre de couches de neurones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "[1, 100] loss: 1.9088922065496445\n",
      "[1, 200] loss: 0.3745897134393454\n",
      "[1, 300] loss: 0.17896830633282662\n",
      "[1, 400] loss: 0.13307213144376873\n",
      "[1, 500] loss: 0.12609489487484096\n",
      "[1, 600] loss: 0.10703282654285431\n",
      "[1, 700] loss: 0.09714987510815262\n",
      "[1, 800] loss: 0.08491703028790652\n",
      "[1, 900] loss: 0.08403366690501571\n",
      "[2, 100] loss: 0.06491851876955479\n",
      "[2, 200] loss: 0.06315013991901651\n",
      "[2, 300] loss: 0.07199754202272743\n",
      "[2, 400] loss: 0.07205491409637034\n",
      "[2, 500] loss: 0.05694212828762829\n",
      "[2, 600] loss: 0.05835183144547045\n",
      "[2, 700] loss: 0.05182410222943872\n",
      "[2, 800] loss: 0.05456122806528583\n",
      "[2, 900] loss: 0.06427852617576718\n",
      "Accuracy of the network on the 10000 test images: 98 %\n",
      "Accuracy for class: 0     is 99.4 %\n",
      "Accuracy for class: 1     is 99.0 %\n",
      "Accuracy for class: 2     is 99.3 %\n",
      "Accuracy for class: 3     is 99.1 %\n",
      "Accuracy for class: 4     is 98.2 %\n",
      "Accuracy for class: 5     is 98.3 %\n",
      "Accuracy for class: 6     is 97.5 %\n",
      "Accuracy for class: 7     is 98.4 %\n",
      "Accuracy for class: 8     is 98.4 %\n",
      "Accuracy for class: 9     is 98.1 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net4 = Net()\n",
    "print(net4)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1309,), (0.3084,)) # Moyenne et ecart-type calculées auparavant\n",
    "])\n",
    "trainset = datasets.MNIST(root='./data', train=True,download=True, transform=transform) # On crée un dataset\n",
    "trainloader = DataLoader(trainset, batch_size=64,shuffle=True) # On crée un dataloader\n",
    "testset = datasets.MNIST(root='./data', train=False,download=True, transform=transform) # On crée un dataset\n",
    "testloader = DataLoader(testset, batch_size=64,shuffle=False) # On crée un dataloader\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net4.parameters(), lr=0.010, momentum=0.9)\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader): # On itère sur les batchs\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad() # On met les gradients à 0\n",
    "        # forward + backward + optimize\n",
    "        outputs = net4(inputs) # On calcule les sorties du réseau\n",
    "        loss = criterion(outputs, labels) # On calcule la loss\n",
    "        loss.backward() # On calcule les gradients\n",
    "        optimizer.step() # On met à jour les poids\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99: # On affiche la loss tous les 100 batchs\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 0.3084 + 0.1309     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calcule de la précision (accuracy) sur le jeu de test\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net4(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "# Accuracy par classe\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net4(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T15:28:39.698965028Z",
     "start_time": "2023-10-16T15:28:26.977289041Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On voit qu'avec un learing rate de 0.01 le réseau de neurones est meilleur dans ses predictions de quelques pourcents comparé a un learing rate de 0.001.\n",
    "Néanmoins, si l'on augmente à plus ont de moins en moins bons résultats. Il faut donc trouver un juste milieu."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
